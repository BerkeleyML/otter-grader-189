{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otter-Grader Demo Notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use otter-grader. For installation details, see the [README](https://github.com/ucbds-infra/otter-grader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from subprocess import PIPE\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebooks\n",
    "\n",
    "Otter supports in-notebook checks so that students can check their progress when working through an assignments. To use it, you need to create an instance of `otter.Notebook`, to which you (optionally) pass a path to a directory of tests; this defaults to `./tests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check a student's work, use `Notebook.check`. Pass to this the question identifier, which is the filename (without the `.py` extension). For example, in this demo, `./tests/q4.py` tests a student's `square` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test = {\n",
      "\t\"name\": \"q4\",\n",
      "\t\"points\": 1,\n",
      "\t\"suites\": [ \n",
      "\t\t{\n",
      "\t\t\t\"cases\": [ \n",
      "\t\t\t\t{\n",
      "\t\t\t\t\t\"code\": r\"\"\"\n",
      "\t\t\t\t\t>>> square(5)\n",
      "\t\t\t\t\t25\n",
      "\t\t\t\t\t>>> square(2.5)\n",
      "\t\t\t\t\t6.25\n",
      "\t\t\t\t\t\"\"\"\n",
      "\t\t\t\t},\n",
      "\t\t\t],\n",
      "\t\t\t\"scored\": False,\n",
      "\t\t\t\"setup\": \"\",\n",
      "\t\t\t\"teardown\": \"\",\n",
      "\t\t\t\"type\": \"doctest\"\n",
      "\t\t}, \n",
      "\t]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./tests/q4.py\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have the square function defined below, we can run the test using `grader.check(\"q4\")`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<otter.gofer.OKTestsResult at 0x112694940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change the function so that the tests fail, then the grader outputs the failed test and the incorrect output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>0 of 1 tests passed</p>\n",
       "        \n",
       "        \n",
       "        <p> <strong>Tests failed: </strong>\n",
       "            <ul>\n",
       "            \n",
       "                <li> \n",
       "    <p><strong style='color: red;'>./tests/q4.py</strong></p>\n",
       "    <p><strong>Test code:</strong><pre><div class=\"highlight\" style=\"background: #f8f8f8\"><pre style=\"line-height: 125%\"><span></span><span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>square(<span style=\"color: #666666\">5</span>)\n",
       "<span style=\"color: #888888\">25</span>\n",
       "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>square(<span style=\"color: #666666\">2.5</span>)\n",
       "<span style=\"color: #888888\">6.25</span>\n",
       "</pre></div>\n",
       "</pre></p>\n",
       "    <p><strong>Test result:</strong><pre>Trying:\n",
       "    square(5)\n",
       "Expecting:\n",
       "    25\n",
       "**********************************************************************\n",
       "Line 2, in ./tests/q4.py 0\n",
       "Failed example:\n",
       "    square(5)\n",
       "Expected:\n",
       "    25\n",
       "Got:\n",
       "    125\n",
       "Trying:\n",
       "    square(2.5)\n",
       "Expecting:\n",
       "    6.25\n",
       "**********************************************************************\n",
       "Line 4, in ./tests/q4.py 0\n",
       "Failed example:\n",
       "    square(2.5)\n",
       "Expected:\n",
       "    6.25\n",
       "Got:\n",
       "    15.625\n",
       "</pre></p>\n",
       "     </li>\n",
       "            \n",
       "            </ul>\n",
       "        \n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<otter.gofer.OKTestsResult at 0x1194c3320>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square = lambda x: x**3\n",
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students can also run all tests at once using `Notebook.check_all`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>q1</strong></p>\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>q2</strong></p>\n",
       "    \n",
       "    \n",
       "        <p>0 of 1 tests passed</p>\n",
       "        \n",
       "        \n",
       "        <p> <strong>Tests failed: </strong>\n",
       "            <ul>\n",
       "            \n",
       "                <li> \n",
       "    <p><strong style='color: red;'>./tests/q2.py</strong></p>\n",
       "    <p><strong>Test code:</strong><pre><div class=\"highlight\" style=\"background: #f8f8f8\"><pre style=\"line-height: 125%\"><span></span><span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span><span style=\"color: #666666\">1</span> <span style=\"color: #666666\">==</span> <span style=\"color: #666666\">1</span>\n",
       "<span style=\"color: #888888\">False</span>\n",
       "</pre></div>\n",
       "</pre></p>\n",
       "    <p><strong>Test result:</strong><pre>Trying:\n",
       "    1 == 1\n",
       "Expecting:\n",
       "    False\n",
       "**********************************************************************\n",
       "Line 2, in ./tests/q2.py 0\n",
       "Failed example:\n",
       "    1 == 1\n",
       "Expected:\n",
       "    False\n",
       "Got:\n",
       "    True\n",
       "</pre></p>\n",
       "     </li>\n",
       "            \n",
       "            </ul>\n",
       "        \n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>q3</strong></p>\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>q4</strong></p>\n",
       "    \n",
       "    \n",
       "        <p>0 of 1 tests passed</p>\n",
       "        \n",
       "        \n",
       "        <p> <strong>Tests failed: </strong>\n",
       "            <ul>\n",
       "            \n",
       "                <li> \n",
       "    <p><strong style='color: red;'>./tests/q4.py</strong></p>\n",
       "    <p><strong>Test code:</strong><pre><div class=\"highlight\" style=\"background: #f8f8f8\"><pre style=\"line-height: 125%\"><span></span><span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>square(<span style=\"color: #666666\">5</span>)\n",
       "<span style=\"color: #888888\">25</span>\n",
       "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>square(<span style=\"color: #666666\">2.5</span>)\n",
       "<span style=\"color: #888888\">6.25</span>\n",
       "</pre></div>\n",
       "</pre></p>\n",
       "    <p><strong>Test result:</strong><pre>Trying:\n",
       "    square(5)\n",
       "Expecting:\n",
       "    25\n",
       "**********************************************************************\n",
       "Line 2, in ./tests/q4.py 0\n",
       "Failed example:\n",
       "    square(5)\n",
       "Expected:\n",
       "    25\n",
       "Got:\n",
       "    125\n",
       "Trying:\n",
       "    square(2.5)\n",
       "Expecting:\n",
       "    6.25\n",
       "**********************************************************************\n",
       "Line 4, in ./tests/q4.py 0\n",
       "Failed example:\n",
       "    square(2.5)\n",
       "Expected:\n",
       "    6.25\n",
       "Got:\n",
       "    15.625\n",
       "</pre></p>\n",
       "     </li>\n",
       "            \n",
       "            </ul>\n",
       "        \n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p><strong>q5</strong></p>\n",
       "    \n",
       "    \n",
       "        <p>0 of 1 tests passed</p>\n",
       "        \n",
       "        \n",
       "        <p> <strong>Tests failed: </strong>\n",
       "            <ul>\n",
       "            \n",
       "                <li> \n",
       "    <p><strong style='color: red;'>./tests/q5.py</strong></p>\n",
       "    <p><strong>Test code:</strong><pre><div class=\"highlight\" style=\"background: #f8f8f8\"><pre style=\"line-height: 125%\"><span></span><span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>negate(<span style=\"color: #666666\">1</span> <span style=\"color: #666666\">==</span> <span style=\"color: #666666\">1</span>)\n",
       "<span style=\"color: #888888\">False</span>\n",
       "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>negate(<span style=\"color: #008000\">True</span> <span style=\"color: #666666\">^</span> <span style=\"color: #008000\">False</span>)\n",
       "<span style=\"color: #888888\">False</span>\n",
       "<span style=\"color: #000080; font-weight: bold\">&gt;&gt;&gt; </span>negate(<span style=\"color: #008000\">True</span> <span style=\"color: #666666\">^</span> <span style=\"color: #008000\">True</span>)\n",
       "<span style=\"color: #888888\">True</span>\n",
       "</pre></div>\n",
       "</pre></p>\n",
       "    <p><strong>Test result:</strong><pre>Trying:\n",
       "    negate(1 == 1)\n",
       "Expecting:\n",
       "    False\n",
       "**********************************************************************\n",
       "Line 2, in ./tests/q5.py 0\n",
       "Failed example:\n",
       "    negate(1 == 1)\n",
       "Exception raised:\n",
       "    Traceback (most recent call last):\n",
       "      File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/doctest.py\", line 1330, in __run\n",
       "        compileflags, 1), test.globs)\n",
       "      File \"<doctest ./tests/q5.py 0[0]>\", line 1, in <module>\n",
       "        negate(1 == 1)\n",
       "    NameError: name 'negate' is not defined\n",
       "Trying:\n",
       "    negate(True ^ False)\n",
       "Expecting:\n",
       "    False\n",
       "**********************************************************************\n",
       "Line 4, in ./tests/q5.py 0\n",
       "Failed example:\n",
       "    negate(True ^ False)\n",
       "Exception raised:\n",
       "    Traceback (most recent call last):\n",
       "      File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/doctest.py\", line 1330, in __run\n",
       "        compileflags, 1), test.globs)\n",
       "      File \"<doctest ./tests/q5.py 0[1]>\", line 1, in <module>\n",
       "        negate(True ^ False)\n",
       "    NameError: name 'negate' is not defined\n",
       "Trying:\n",
       "    negate(True ^ True)\n",
       "Expecting:\n",
       "    True\n",
       "**********************************************************************\n",
       "Line 6, in ./tests/q5.py 0\n",
       "Failed example:\n",
       "    negate(True ^ True)\n",
       "Exception raised:\n",
       "    Traceback (most recent call last):\n",
       "      File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/doctest.py\", line 1330, in __run\n",
       "        compileflags, 1), test.globs)\n",
       "      File \"<doctest ./tests/q5.py 0[2]>\", line 1, in <module>\n",
       "        negate(True ^ True)\n",
       "    NameError: name 'negate' is not defined\n",
       "</pre></p>\n",
       "     </li>\n",
       "            \n",
       "            </ul>\n",
       "        \n",
       "    \n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students can also create their own PDF submissions using `Notebook.export`, to which you pass the path to the notebook. Otter uses nb2pdf to generate PDFs (more info [below](#nb2pdf)). The filtering behavior of nb2pdf is turned on by default, although it can be turned off using the `filtering=False` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t<p>Your file has been exported. Download it \n",
       "\t\t<a href=\"demo.pdf\" target=\"_blank\">here</a>!\n",
       "\t\t"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grader.export(\"demo.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts\n",
    "\n",
    "If a student is working with Python scripts instead of notebooks, the otter command line utility has a `check` command that can run these tests as well. The help entry for this command is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: otter [-h] [-q QUESTION] [-t TESTS-PATH] file\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  file           Python file to grade\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help     show this help message and exit\r\n",
      "  -q QUESTION    Grade a specific test\r\n",
      "  -t TESTS-PATH  Path to test files\r\n"
     ]
    }
   ],
   "source": [
    "!otter check -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to run a single check, we would pass the question identifier (the filename without `.py`) to the `-q` flag. The tests path is assumed to be `./tests` unless another path is provided to the `-t` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to revert mtime: /Library/Fonts\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "!otter check scripts/file0.py -q q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with notebooks, a failed test will output the test and the incorrect output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 2 tests passed\r\n",
      "\r\n",
      "Tests passed:\r\n",
      " possible \r\n",
      "\r\n",
      "\r\n",
      "Tests failed: \r\n",
      "*********************************************************************\r\n",
      "Line 2, in tests/q2.py 0\r\n",
      "Failed example:\r\n",
      "    1 == 1\r\n",
      "Expected:\r\n",
      "    False\r\n",
      "Got:\r\n",
      "    True\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!otter check scripts/file0.py -q q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check all of the tests at once, use `otter check` without a `-q` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 of 6 tests passed\r\n",
      "\r\n",
      "Tests passed:\r\n",
      " q1  q3  q4  q5 \r\n",
      "\r\n",
      "\r\n",
      "Tests failed: \r\n",
      "*********************************************************************\r\n",
      "Line 2, in tests/q2.py 0\r\n",
      "Failed example:\r\n",
      "    1 == 1\r\n",
      "Expected:\r\n",
      "    False\r\n",
      "Got:\r\n",
      "    True\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!otter check scripts/file0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebooks\n",
    "\n",
    "Otter uses Docker containers to execute students' submissions in parallel containers. It accepts exports from Gradescope and Canvas, or the notebook files with a JSON- or YAML-formatted metadata file. If you're using the custom metadata file, it requires an entry for each notebook, with the notebook filename as the `filename` parameter and the student identifier as `identifier`. An example of the YAML metadata can be found in `notebooks/meta.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- identifier: 0\n",
      "  filename: test-nb-0.ipynb\n",
      "- identifier: 1\n",
      "  filename: test-nb-1.ipynb\n",
      "- identifier: 2\n",
      "  filename: test-nb-2.ipynb\n",
      "- identifier: 3\n",
      "  filename: test-nb-3.ipynb\n",
      "- identifier: 4\n",
      "  filename: test-nb-4.ipynb\n"
     ]
    }
   ],
   "source": [
    "with open(\"./notebooks/meta.yml\") as f:\n",
    "    for l in f.readlines()[0:10]:\n",
    "        print(l[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "\n",
    "The docker container comes with the following packages installed:\n",
    "\n",
    "* datascience\n",
    "* jupyter_client\n",
    "* ipykernel\n",
    "* matplotlib\n",
    "* pandas\n",
    "* ipywidgets\n",
    "* scipy\n",
    "* nb2pdf\n",
    "* otter-grader\n",
    "\n",
    "If you have any other requirements besides these, create a `requirements.txt` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./requirements.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command-Line Usage\n",
    "\n",
    "Now let's examine the usage of the `otter` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: otter [-h] [-p NOTEBOOKS-PATH] [-t TESTS-PATH] [-o OUTPUT-PATH] [-g]\r\n",
      "             [-c] [-j JSON] [-y YAML] [-s] [--pdf] [--filter-pdf]\r\n",
      "             [-f FILES [FILES ...]] [-v] [-r REQUIREMENTS]\r\n",
      "             [--containers NUM-CONTAINERS] [--image IMAGE]\r\n",
      "\r\n",
      "Local autograder for Jupyter Notebooks and Python scripts. Launches parallel\r\n",
      "Docker containers to grade notebooks/scripts and returns a CSV of grades.\r\n",
      "Requires a metadata file if not exported from Gradescope or Canvas. Add\r\n",
      "support files by putting them into the SUBMISSIONS-DIR folder or using the -f\r\n",
      "flag. Can optionally create PDFs of Jupyter Notebooks for manual grading.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -p NOTEBOOKS-PATH, --path NOTEBOOKS-PATH\r\n",
      "                        Path to directory of submissions\r\n",
      "  -t TESTS-PATH, --tests-path TESTS-PATH\r\n",
      "                        Path to directory of tests\r\n",
      "  -o OUTPUT-PATH, --output-path OUTPUT-PATH\r\n",
      "                        Path to which to write output\r\n",
      "  -g, --gradescope      Flag for Gradescope export\r\n",
      "  -c, --canvas          Flag for Canvas export\r\n",
      "  -j JSON, --json JSON  Flag for path to JSON metadata\r\n",
      "  -y YAML, --yaml YAML  Flag for path to YAML metadata\r\n",
      "  -s, --scripts         Flag to incidicate grading Python scripts\r\n",
      "  --pdf                 Create unfiltered PDFs for manual grading\r\n",
      "  --filter-pdf          Create filtered PDF for manual grading\r\n",
      "  -f FILES [FILES ...], --files FILES [FILES ...]\r\n",
      "                        Specify support files needed to execute code (e.g.\r\n",
      "                        utils, data files)\r\n",
      "  -v, --verbose         Flag for verbose output\r\n",
      "  -r REQUIREMENTS, --requirements REQUIREMENTS\r\n",
      "                        Flag for Python requirements file path\r\n",
      "  --containers NUM-CONTAINERS\r\n",
      "                        Specify number of containers to run in parallel\r\n",
      "  --image IMAGE         Custom docker image to run on\r\n"
     ]
    }
   ],
   "source": [
    "!otter -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-p` flag should be the path to the directory that conains the notebooks (or the unzipped export from Canvas or Gradescope). The `g`, `c`, `y`, and `j` flags indicate the metadata type; the `y` and `j` flags require an argument that indicates the path to the metadata file. If you want to generate PDFs for manual grading, you can add the `--pdf` or `--filter-pdf` flags, which generate an unfiltered or filtered, respectively, PDF for each notebook. If you need a requirements file, pass that path to the `-r` flag. Optionally, you can set the number of docker containers or the docker image that the grader runs on using the `--containers` and `--image` flags, respectively.\n",
    "\n",
    "Some flag defaults are listed below. If a flag is not list, it has no default value.\n",
    "\n",
    "| Flag | Default |\n",
    "|-----|-----|\n",
    "| `-p` | `./` |\n",
    "| `-t` | `./tests` |\n",
    "| `-o` | `./` |\n",
    "| `--containers` | 4 |\n",
    "| `--image` | `ucbdsinfra/otter-grader` |\n",
    "\n",
    "**If your notebooks require any other files (e.g. data files)**, place these files into the notebooks path directory (whatever the value of `-p` is). These will automatically be copied into the docker containers.\n",
    "\n",
    "Let's run the autograder on the files in `./notebooks`. The command below tells otter that our metadata file can be found at `./notebooks/meta.yml`, that the notebooks can be found in `./notebooks`, that we have a requirements file at `./requirements.txt`, and to use verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found YAML metadata...\n",
      "Launching docker containers...\n",
      "Launched container 1b4b6bf6b9a3...\n",
      "Launched container 52a50e11981c...\n",
      "Launched container 36aa3362f5ca...\n",
      "Launched container 03eba9df2e0e...\n",
      "Installing requirements in container 1b4b6bf6b9a3...\n",
      "Installing requirements in container 52a50e11981c...\n",
      "Installing requirements in container 36aa3362f5ca...\n",
      "Installing requirements in container 03eba9df2e0e...\n",
      "Grading notebooks in container 52a50e11981c...\n",
      "Grading notebooks in container 03eba9df2e0e...\n",
      "Grading notebooks in container 1b4b6bf6b9a3...\n",
      "Grading notebooks in container 36aa3362f5ca...\n",
      "Copying grades from container 03eba9df2e0e...\n",
      "Copying grades from container 36aa3362f5ca...\n",
      "Copying grades from container 52a50e11981c...\n",
      "Copying grades from container 1b4b6bf6b9a3...\n",
      "Stopping container 36aa3362f5ca...\n",
      "Stopping container 03eba9df2e0e...\n",
      "Stopping container 52a50e11981c...\n",
      "Stopping container 1b4b6bf6b9a3...\n",
      "Combining grades and saving...\n"
     ]
    }
   ],
   "source": [
    "!otter -y notebooks/meta.yml -p notebooks -r requirements.txt -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have a CSV file with the scores and score breakdowns at `./final_grades.csv` (since we left `-o` with its default value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>file</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>q4</th>\n",
       "      <th>q5</th>\n",
       "      <th>total</th>\n",
       "      <th>possible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>test-nb-52.ipynb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>test-nb-9.ipynb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>test-nb-11.ipynb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>test-nb-2.ipynb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>test-nb-21.ipynb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identifier              file   q1   q2   q3   q4   q5  total  possible\n",
       "0          52  test-nb-52.ipynb  1.0  0.0  1.0  0.0  0.0    2.0         5\n",
       "1           9   test-nb-9.ipynb  1.0  0.0  1.0  0.0  0.0    2.0         5\n",
       "2          11  test-nb-11.ipynb  1.0  0.0  1.0  0.0  0.0    2.0         5\n",
       "3           2   test-nb-2.ipynb  1.0  0.0  1.0  0.0  0.0    2.0         5\n",
       "4          21  test-nb-21.ipynb  1.0  0.0  1.0  0.0  0.0    2.0         5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"final_grades.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the autograder ran correctly, all of the submissions should receive a 2/5, since three of the tests are written to fail on these notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nb2pdf\n",
    "\n",
    "When exporting notebook PDFs, otter uses the library nb2pdf, which in turn relies on nbpdfexporter. This requires that chromium be installed on the docker container (which is true if using `ucbdsinfra/otter-grader`, the default image) but it also must be installed on the JupyterHub distribution that students use if they are going to be exporting PDFs themselves.\n",
    "\n",
    "nb2pdf can filter notebooks in two ways: using cell tags or using HTML comments in Markdown cells.\n",
    "\n",
    "##### Tags\n",
    "\n",
    "When generating the PDF (if filtering is turned on, either by use of the `--tag-filter` flag or `filter_type=\"tags\"` in `Notebook.export`), the following cells will be included by default:\n",
    "\n",
    "* Markdown cells\n",
    "* Code cells with images in the output\n",
    "* All cells tagged with `include`\n",
    "\n",
    "If a cell falls within one of the 3 categories listed above but you do not want to include it in the exported PDF, tag the cell with `ignore`. This cell will then not be added to the manual graded PDF.\n",
    "\n",
    "##### HTML Comments\n",
    "\n",
    "If you elect to use HTML comments, all content that is between the HTML comments `<!-- BEGIN QUESTION -->` and `<!-- END QUESTION -->` will be included. This filtering is triggered with the `--html-filter` PDF and is the default behavior of `Notebook.export`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts\n",
    "\n",
    "Otter can also grade Python scripts using the addition of the `-s` flag. It uses the same metadata and path structure as with notebooks, so the only change to our call above will be to change the path to `./scripts`. *This will overwrite `final_grades.csv` from above since we aren't changing the output path.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found YAML metadata...\n",
      "Launching docker containers...\n",
      "Launched container 79a0856c4dda...\n",
      "Launched container d186eeff8a4e...\n",
      "Launched container 772fcbbc1c2d...\n",
      "Launched container 5304474cae57...\n",
      "Installing requirements in container 79a0856c4dda...\n",
      "Installing requirements in container d186eeff8a4e...\n",
      "Installing requirements in container 5304474cae57...\n",
      "Installing requirements in container 772fcbbc1c2d...\n",
      "Grading scripts in container d186eeff8a4e...\n",
      "Grading scripts in container 79a0856c4dda...\n",
      "Grading scripts in container 5304474cae57...\n",
      "Grading scripts in container 772fcbbc1c2d...\n",
      "Copying grades from container 79a0856c4dda...\n",
      "Stopping container 79a0856c4dda...\n",
      "Copying grades from container 5304474cae57...\n",
      "Copying grades from container d186eeff8a4e...\n",
      "Copying grades from container 772fcbbc1c2d...\n",
      "Stopping container 5304474cae57...\n",
      "Stopping container d186eeff8a4e...\n",
      "Stopping container 772fcbbc1c2d...\n",
      "Combining grades and saving...\n"
     ]
    }
   ],
   "source": [
    "!otter -sy scripts/meta.yml -p scripts -r requirements.txt -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should be able to read in the grades from the scripts. If all went well, they should each have a 4/5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>file</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>q4</th>\n",
       "      <th>q5</th>\n",
       "      <th>total</th>\n",
       "      <th>possible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>file12.py</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>file71.py</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>file78.py</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84</td>\n",
       "      <td>file84.py</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>file28.py</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identifier       file   q1   q2   q3   q4   q5  total  possible\n",
       "0          12  file12.py  1.0  0.0  1.0  1.0  1.0    4.0         5\n",
       "1          71  file71.py  1.0  0.0  1.0  1.0  1.0    4.0         5\n",
       "2          78  file78.py  1.0  0.0  1.0  1.0  1.0    4.0         5\n",
       "3          84  file84.py  1.0  0.0  1.0  1.0  1.0    4.0         5\n",
       "4          28  file28.py  1.0  0.0  1.0  1.0  1.0    4.0         5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"final_grades.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradescope\n",
    "\n",
    "Otter is compatible with the Gradescope autograder, and has a command line tool to generate the zipfile that is used to configure the Docker container that Gradescope's autograder usage. The base command for this utility is `otter gen`, and its help entry is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: otter [-h] [-t [TESTS-PATH]] [-o [OUTPUT-PATH]] [-r [REQUIREMENTS]]\r\n",
      "             [--threshold THRESHOLD] [--points POINTS]\r\n",
      "             [files [files ...]]\r\n",
      "\r\n",
      "Generates zipfile to configure Gradescope autograder\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  files                 Other support files needed for grading (e.g. .py\r\n",
      "                        files, data files)\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -t [TESTS-PATH], --tests-path [TESTS-PATH]\r\n",
      "                        Path to test files\r\n",
      "  -o [OUTPUT-PATH], --output-path [OUTPUT-PATH]\r\n",
      "                        Path to which to write zipfile\r\n",
      "  -r [REQUIREMENTS], --requirements [REQUIREMENTS]\r\n",
      "                        Path to requirements.txt file\r\n",
      "  --threshold THRESHOLD\r\n",
      "                        Pass/fail score threshold\r\n",
      "  --points POINTS       Points possible, overrides sum of test points\r\n"
     ]
    }
   ],
   "source": [
    "!otter gen -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `otter gen` command creates a zipfile at `OUTPUT-PATH/autograder.zip` which contains the necessary files for the Gradescope autograder:\n",
    "\n",
    "* `run_autograder`: the script that executes otter\n",
    "* `setup.sh`: a file that instructs Ubuntu on how to install dependencies\n",
    "* `requirements.txt`: Python's list of necessary dependencies\n",
    "* `tests`: the folder of test cases\n",
    "* `files`: a folder containing any files needed for the notebooks to execute (e.g. data files)\n",
    "\n",
    "The requirements file create automatically includes the otter dependencies (see [Requirements](#Requirements)), but you can optionally include your own, other ones by passing a filepath to the `-r` flag. Any files included that are not passed to a flag are automatically placed into the `files` folder in the zipfile and will be copied into the notebook directory in the Gradescope container.\n",
    "\n",
    "Let's generate a zipfile from this directory with the tests in `./tests`, the requirements in `./requirements.txt`, and `test-df.csv` as a data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!otter gen -r requirements.txt test-df.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a file `./autograder.zip` in the working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"autograder.zip\" in os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we unzip the file, you'll see all of the files listed above as well as the CSV file we added in the `autograder/files` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  autograder.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unzipped = subprocess.run([\"unzip\", \"autograder.zip\", \"-d\", \"autograder\"], stdout=PIPE, stderr=PIPE)\n",
    "print(unzipped.stdout.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setup.sh', 'requirements.txt', 'run_autograder', 'tests', 'files']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"autograder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass/Fail Thresholds\n",
    "\n",
    "The generator also supports scoring assignments on a pass/fail basis using only some percentage threshold. If you include a threshold, pass a number between 0 and 1 to the `--threshold` flag. If a student scores below `--threshold`, then they will get 0 points; otherwise, they will get points for all questions.\n",
    "\n",
    "The command below creates an autograder that automatically gives students full points as long as they get at least an 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!otter gen -r requirements.txt test-df.csv --threshold 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overriding Points Possible\n",
    "\n",
    "Otter also supports overriding the number of points possible for an assignment from the default to the sum of the points possible for each test using the `--points` flag. This is done by scaling the new points possible by the percentage of points received as a decimal. For example, if a student passes a 2- and 1-point test but fails a 4-point test on an assignment with `--points` set to 2, they will receive a $\\frac{2 + 1}{2 + 1 + 4} \\cdot 2 = 0.85714285714$.\n",
    "\n",
    "The command below creates an autograder that scales the number of points down to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!otter gen -r requirements.txt test-df.csv --points 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative Imports on Gradescope\n",
    "\n",
    "Because of the way that the Gradescope autograder is set up, our script must change relative import syntax in order to ensure that the necessary files are loaded. **Because we cannot differentiate between package imports and relative imports, `otter gen` automatically assumes that if you are importing a .py file that it is called `utils.py`.** The regex used to change the import syntax will _only_ change the syntax if you're using the import statment\n",
    "\n",
    "```python\n",
    "from utils import *\n",
    "```\n",
    "\n",
    "For this reason, if you have any functions you need in a file that is going to be imported, _make sure that it is called `utils.py`._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
