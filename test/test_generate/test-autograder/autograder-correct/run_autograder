#!/usr/bin/env python3

import json
import os
import shutil
import subprocess
import re
import pickle
import warnings
import pandas as pd

from glob import glob
from textwrap import dedent
from nb2pdf import convert

from otter.execute import grade_notebook
from otter.export import export_notebook
from otter.generate.token import APIClient
from otter.logs import Log, QuestionNotInLogException
from otter.notebook import _OTTER_LOG_FILENAME

SCORE_THRESHOLD = None
POINTS_POSSIBLE = None
SHOW_STDOUT_ON_RELEASE = False
SHOW_HIDDEN_TESTS_ON_RELEASE = False
SEED = None
GRADE_FROM_LOG = False
SERIALIZED_VARIABLES = {}
PUBLIC_TEST_MULTIPLIER = 0

# for auto-uploading PDFs
TOKEN = None
COURSE_ID = 'None'
ASSIGNMENT_ID = 'None'
FILTERING = True
PAGEBREAKS = True

if TOKEN is not None:
    CLIENT = APIClient(token=TOKEN)
    GENERATE_PDF = True
else:
    GENERATE_PDF = False

UTILS_IMPORT_REGEX = r"\"from utils import [\w\*, ]+"
NOTEBOOK_INSTANCE_REGEX = r"otter.Notebook\(.+\)"

if __name__ == "__main__":
    # put files into submission directory
    if os.path.exists("/autograder/source/files"):
        for filename in glob("/autograder/source/files/*.*"):
            shutil.copy(filename, "/autograder/submission")

    # create __init__.py files
    subprocess.run(["touch", "/autograder/__init__.py"])
    subprocess.run(["touch", "/autograder/submission/__init__.py"])

    os.chdir("/autograder/submission")

    # check for *.ipynb.json files
    jsons = glob("*.ipynb.json")
    for file in jsons:
        shutil.copy(file, file[:-5])

    # check for *.ipynb.html files
    htmls = glob("*.ipynb.html")
    for file in htmls:
        shutil.copy(file, file[:-5])

    nb_path = glob("*.ipynb")[0]

    # fix utils import
    try:
        with open(nb_path) as f:
            contents = f.read()
    except UnicodeDecodeError:
        with open(nb_path, "r", encoding="utf-8") as f:
            contents = f.read()

    # contents = re.sub(UTILS_IMPORT_REGEX, "\"from .utils import *", contents)
    contents = re.sub(NOTEBOOK_INSTANCE_REGEX, "otter.Notebook()", contents)

    try:
        with open(nb_path, "w") as f:
            f.write(contents)
    except UnicodeEncodeError:
        with open(nb_path, "w", encoding="utf-8") as f:
            f.write(contents)

    try:
        os.mkdir("/autograder/submission/tests")
    except FileExistsError:
        pass

    tests_glob = glob("/autograder/source/tests/*.py")
    for file in tests_glob:
        shutil.copy(file, "/autograder/submission/tests")

    if glob("*.otter"):
        assert len(glob("*.otter")) == 1, "Too many .otter files (max 1 allowed)"
        with open(glob("*.otter")[0]) as f:
            config = json.load(f)
    else:
        config = None

    if GRADE_FROM_LOG:
        assert os.path.isfile(_OTTER_LOG_FILENAME), "missing log"
        log = Log.from_file(_OTTER_LOG_FILENAME, ascending=False)
        print("\n\n")     # for logging in otter.execute.execute_log
    else:
        log = None

    scores = grade_notebook(
        nb_path, 
        glob("/autograder/submission/tests/*.py"), 
        name="submission", 
        cwd="/autograder/submission", 
        test_dir="/autograder/submission/tests",
        ignore_errors=True, 
        seed=SEED,
        log=log
    )
    # del scores["TEST_HINTS"]

    # verify the scores against the log
    print("\n\n")
    if os.path.isfile(_OTTER_LOG_FILENAME):
        log = Log.from_file(_OTTER_LOG_FILENAME, ascending=False)
        try:
            found_discrepancy = log.verify_scores(scores)
            if not found_discrepancy:
                print("No discrepancies found while verifying scores against the log.")
        except BaseException as e:
            print(f"Error encountered while trying to verify scores with log:\n{e}")
    else:
        print("No log found with which to verify student scores")

    if GENERATE_PDF:
        try:
            export_notebook(nb_path, filtering=FILTERING, pagebreaks=PAGEBREAKS)
            pdf_path = os.path.splitext(nb_path)[0] + ".pdf"

            # get student email
            with open("../submission_metadata.json") as f:
                metadata = json.load(f)

            student_emails = []
            for user in metadata["users"]:
                student_emails.append(user["email"])
            
            for student_email in student_emails:
                CLIENT.upload_pdf_submission(COURSE_ID, ASSIGNMENT_ID, student_email, pdf_path)

            print("\n\nSuccessfully uploaded submissions for: {}".format(", ".join(student_emails)))

        except:
            print("\n\n")
            warnings.warn("PDF generation or submission failed", RuntimeWarning)

    # hidden visibility determined by SHOW_HIDDEN_TESTS_ON_RELEASE
    hidden_test_visibility = ("hidden", "after_published")[SHOW_HIDDEN_TESTS_ON_RELEASE]

    output = {"tests" : []}
    for key in scores:
        if key != "total" and key != "possible":
            hidden, incorrect = scores[key].get("hidden", False), "hint" in scores[key]
            score, possible = scores[key]["score"], scores[key]["possible"]
            public_score, hidden_score = score * PUBLIC_TEST_MULTIPLIER, score * (1 - PUBLIC_TEST_MULTIPLIER)
            public_possible, hidden_possible = possible * PUBLIC_TEST_MULTIPLIER, possible * (1 - PUBLIC_TEST_MULTIPLIER)
            
            output["tests"] += [{
                "name" : key + " - Public",
                "score" : (public_score, score)[not hidden and incorrect],
                "max_score": (public_possible, possible)[not hidden and incorrect],
                "visibility": "visible",
                "output": repr(scores[key]["test"]),
            }]
            # if not hidden and incorrect:
            #     output["tests"][-1]["output"] = repr(scores[key]["hint"])
            
            if not (not hidden and incorrect):
                output["tests"] += [{
                    "name" : key + " - Hidden",
                    "score" : (score, hidden_score)[not hidden and incorrect],
                    "max_score": (possible, hidden_possible)[not hidden and incorrect],
                    "visibility": hidden_test_visibility,
                    "output": repr(scores[key]["test"])
                }]
                # if hidden and incorrect:
                #     output["tests"][-1]["output"] = repr(scores[key]["hint"])
    
    if SHOW_STDOUT_ON_RELEASE:
        output["stdout_visibility"] = "after_published"

    if POINTS_POSSIBLE is not None:
        output["score"] = scores["total"] / scores["possible"] * POINTS_POSSIBLE

    if SCORE_THRESHOLD is not None:
        if scores["total"] / scores["possible"] >= SCORE_THRESHOLD:
            output["score"] = POINTS_POSSIBLE or scores["possible"]
        else:
            output["score"] = 0

    with open("/autograder/results/results.json", "w+") as f:
        json.dump(output, f, indent=4)

    print("\n\n")
    print(dedent("""\
    Test scores are summarized in the table below. Passed tests appear as a single cell with no output.
    Failed public tests appear as a single cell with the output of the failed test. Failed hidden tests
    appear as two cells, one with no output (the public tests) and another with the output of the failed
    (hidden) test that is not visible to the student.
    """))
    df = pd.DataFrame(output["tests"])
    if "output" in df.columns:
        df.drop(columns=["output"], inplace=True)
    # df.drop(columns=["hidden"], inplace=True)
    print(df)